{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31402, 56, 56) (31402,)\n",
      "Validation set (2000, 56, 56) (2000,)\n",
      "Test set (13068, 56, 56) (13068,)\n"
     ]
    }
   ],
   "source": [
    "with open('SVHN_train.pickle', 'rb') as f:\n",
    "    tmp_save = pickle.load(f)\n",
    "    train_dataset_56 = tmp_save['dataset']\n",
    "    train_labels = tmp_save['labels']\n",
    "    \n",
    "valid_size = 2000\n",
    "valid_dataset_56 = train_dataset_56[:valid_size]\n",
    "valid_labels = train_labels[:valid_size]\n",
    "train_dataset_56 = train_dataset_56[valid_size:]\n",
    "train_labels = train_labels[valid_size:]\n",
    "\n",
    "with open('SVHN_test.pickle', 'rb') as f:\n",
    "    tmp_save = pickle.load(f)\n",
    "    test_dataset_56 = tmp_save['dataset']\n",
    "    test_labels = tmp_save['labels']\n",
    "    \n",
    "print('Training set', train_dataset_56.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset_56.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset_56.shape, test_labels.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31401, 56, 56) (31401,)\n",
      "Validation set (2000, 56, 56) (2000,)\n",
      "Test set (13068, 56, 56) (13068,)\n"
     ]
    }
   ],
   "source": [
    "num_digits = 5\n",
    "def max_digits(dataset, labels, max_digits):\n",
    "    keep = [i for i, label in enumerate(labels) if len(label) <= max_digits]\n",
    "    return dataset[keep], labels[keep]\n",
    "\n",
    "train_dataset_56, train_labels = max_digits(train_dataset_56, train_labels, 5)\n",
    "valid_dataset_56, valid_labels = max_digits(valid_dataset_56, valid_labels, 5)\n",
    "test_dataset_56, test_labels = max_digits(test_dataset_56, test_labels, 5)\n",
    "print('Training set', train_dataset_56.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset_56.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset_56.shape, test_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image(img, label):\n",
    "    print(\"Labels\", label)\n",
    "    print(\"Dtype\", img.dtype)\n",
    "    print(\"Shape\", img.shape)\n",
    "    print(\"Color range\", np.min(img), np.max(img))\n",
    "    if len(img.shape) > 2:\n",
    "        plt.imshow(np.reshape(img, img.shape[:2]))\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def show_images(imgs, labels, num=3):\n",
    "    for i in range(num):\n",
    "        num = np.random.randint(imgs.shape[0])\n",
    "        show_image(imgs[num], labels[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31401, 56, 56, 1) (31401, 5, 11)\n",
      "Validation set (2000, 56, 56, 1) (2000, 5, 11)\n",
      "Test set (13068, 56, 56, 1) (13068, 5, 11)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 11 # Add an extra character so we can deal with spaces\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset_output = dataset.reshape(list(dataset.shape) + [1]).astype(np.float32)\n",
    "    labels_output = np.array([np.array([(np.arange(num_labels) == l).astype(np.float32) \n",
    "                        for l in np.append(row, [num_labels-1] * (num_digits - len(row)), 0)])\n",
    "              for row in labels])\n",
    "    return dataset_output, labels_output\n",
    "\n",
    "train_dataset_56, train_labels = reformat(train_dataset_56, train_labels)\n",
    "valid_dataset_56, valid_labels = reformat(valid_dataset_56, valid_labels)\n",
    "test_dataset_56, test_labels = reformat(test_dataset_56, test_labels)\n",
    "\n",
    "print('Training set', train_dataset_56.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset_56.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset_56.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (31401, 28, 28, 1) (31401, 5, 11)\n",
      "Validation set (2000, 28, 28, 1) (2000, 5, 11)\n",
      "Test set (6000, 28, 28, 1) (6000, 5, 11)\n"
     ]
    }
   ],
   "source": [
    "#Limit test set as it's killing the box\n",
    "test_dataset_56 = test_dataset_56[:6000]\n",
    "test_labels = test_labels[:6000]\n",
    "\n",
    "# Create a pit sized \n",
    "train_dataset_28 = train_dataset_56[:, ::2, ::2, :]\n",
    "valid_dataset_28 = valid_dataset_56[:, ::2, ::2, :]\n",
    "test_dataset_28 = test_dataset_56[:, ::2, ::2, :]\n",
    "print('Training set', train_dataset_28.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset_28.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset_28.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_56 = (train_dataset_56, valid_dataset_56, test_dataset_56)\n",
    "dataset_28 = (train_dataset_28, valid_dataset_28, test_dataset_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def accuracy_list(predictions, labels):\n",
    "    result = np.mean([accuracy(predictions[i],\n",
    "                               labels[:, i, :])\n",
    "                        for i in range(num_digits)])\n",
    "    return result\n",
    "\n",
    "def run_graph(graph, train_dataset):\n",
    "    if test_graphs: return\n",
    "    start_time = time.time()\n",
    "        \n",
    "    batch_size=16\n",
    "    \n",
    "    num_steps = 1000001\n",
    "    save_steps = 100000\n",
    "    eval_steps = 500\n",
    "    valid_steps = 500\n",
    "    timeout = 1 * 60 # 30 minutes * 60 seconds\n",
    "        \n",
    "    tf_train_dataset = graph.get_tensor_by_name('tf_train_dataset:0')\n",
    "    tf_train_labels = [graph.get_tensor_by_name('tf_train_labels_%d:0' % i) for i in range(num_digits)]\n",
    "    train_prediction = [graph.get_tensor_by_name('train_prediction_%d:0' % i) for i in range(num_digits)]\n",
    "    valid_prediction = [graph.get_tensor_by_name('valid_prediction_%d:0' % i) for i in range(num_digits)]\n",
    "    test_prediction = [graph.get_tensor_by_name('test_prediction_%d:0' % i) for i in range(num_digits)]\n",
    "    optimizer = graph.get_tensor_by_name('optimizer:0')\n",
    "    loss = graph.get_tensor_by_name('loss:0')\n",
    "    learning_rate = graph.get_tensor_by_name('learning_rate:0')\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "\n",
    "            feed_dict = {tf_train_labels[i] : batch_labels[:, i, :] for i in range(num_digits)}\n",
    "            feed_dict[tf_train_dataset] = batch_data        \n",
    "\n",
    "\n",
    "            fetches = [optimizer, loss]\n",
    "            results = session.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            timeup = elapsed_time >= timeout\n",
    "            \n",
    "            if step > 0 or timeup:\n",
    "                if (step % eval_steps == 0 or timeup):\n",
    "                    print('Elapsed time(s):%d/%d (%.2f%%)' % (elapsed_time, timeout, 1.0 * elapsed_time / timeout))\n",
    "                    if timeup:\n",
    "                        print(\"\\nTIMEUP!\")\n",
    "                    print('Learning rate:', learning_rate.eval())\n",
    "                    print('Minibatch loss at step %d: %f' % (step, results[1]))\n",
    "\n",
    "                    fetches = train_prediction\n",
    "                    results = session.run(fetches, feed_dict={tf_train_dataset : batch_data})\n",
    "                    train_accuracy = accuracy_list(\n",
    "                        results,\n",
    "                        batch_labels)\n",
    "\n",
    "                    print('Minibatch accuracy: %.1f%%' % train_accuracy)\n",
    "            \n",
    "                if (step % valid_steps == 0 or timeup):\n",
    "                    valid_accuracy = accuracy_list(\n",
    "                        [valid_prediction[i].eval() for i in range(num_digits)],\n",
    "                        valid_labels)\n",
    "                    print('Validation accuracy: %.1f%%' % valid_accuracy)\n",
    "\n",
    "                if step % save_steps == 0 or timeup:\n",
    "                    test_accuracy = accuracy_list(\n",
    "                        [test_prediction[i].eval() for i in range(num_digits)],\n",
    "                        test_labels)\n",
    "                    print('Test accuracy: %.1f%%' % test_accuracy)\n",
    "                    \n",
    "            if timeup:\n",
    "                break\n",
    "\n",
    "        print(\"Saving graph\")\n",
    "        checkpoint_path = os.path.join('save', 'model.ckpt')\n",
    "        saver.save(session, checkpoint_path, global_step=0)\n",
    "        tf.train.write_graph(session.graph.as_graph_def(), 'save', 'model.pb')\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This graph will start of simple, and get more complex as we try different inputs\n",
    "def create_graph(valid_dataset, test_dataset,\n",
    "                 use_dropout=False,\n",
    "                 learning_rate=0.05,\n",
    "                 learning_decay=0.596,\n",
    "                 use_max_pool=False):\n",
    "    \n",
    "    batch_size = 16\n",
    "    patch_size = 5\n",
    "    depth = 16\n",
    "\n",
    "    # Taken from tensorflow tutorial\n",
    "    num_hidden = 64\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    stddev = 0.1\n",
    "\n",
    "    img_height, img_width = test_dataset[0].shape[:2]\n",
    "\n",
    "    with graph.as_default():\n",
    "\n",
    "        # Input data.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                           img_height,\n",
    "                                                           img_width,\n",
    "                                                           num_channels), name=\"tf_train_dataset\")\n",
    "\n",
    "        tf_train_labels = [tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                             num_labels), name=\"tf_train_labels_%d\" % i)\n",
    "                           for i in range(num_digits)]\n",
    "\n",
    "        # Variables.\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal([patch_size,\n",
    "                                                        patch_size,\n",
    "                                                        num_channels,\n",
    "                                                        depth], stddev=stddev), name=\"layer1_weights\")\n",
    "        layer1_biases = tf.Variable(tf.constant(0.1, shape=[depth]), name=\"layer1_biases\")\n",
    "\n",
    "        tmp_size = img_width * img_height * depth\n",
    "        if use_max_pool: tmp_size = int(tmp_size / 4.)\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal([tmp_size, num_hidden], stddev=stddev),\n",
    "                                     name=\"layer2_weights\")\n",
    "        layer2_biases = tf.Variable(tf.constant(0.1, shape=[num_hidden]), name=\"layer2_biases\")\n",
    "\n",
    "\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal([num_hidden,\n",
    "                                                        num_labels * num_digits], stddev=stddev), name=\"layer4_weights\")\n",
    "        layer4_biases = tf.Variable(tf.ones(shape=[num_labels * num_digits]), name=\"layer4_biases\")\n",
    "\n",
    "        # Model.\n",
    "        def model(data, dropout=False):\n",
    "            if dropout: data = tf.nn.dropout(data, 0.9)\n",
    "\n",
    "            conv = tf.nn.relu(tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases)\n",
    "            if use_max_pool: conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            if dropout: conv = tf.nn.dropout(conv, 0.75)\n",
    "\n",
    "            shape = conv.get_shape().as_list()\n",
    "            reshape = tf.reshape(conv, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer2_weights) + layer2_biases)\n",
    "            if dropout: hidden = tf.nn.dropout(hidden, 0.5)\n",
    "            \n",
    "            output = tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "            split_logits = tf.split(1, num_digits, output)\n",
    "            return split_logits\n",
    "\n",
    "        # Training computation.\n",
    "        logits = model(tf_train_dataset, use_dropout)\n",
    "\n",
    "        # Training computation.\n",
    "        logits = model(tf_train_dataset)\n",
    "\n",
    "        loss = tf.reduce_mean([\n",
    "                            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                logits[i],\n",
    "                                tf_train_labels[i]\n",
    "                            )for i in range(num_digits)], name='loss')\n",
    "\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate, \n",
    "                                                   global_step, \n",
    "                                                   100000, \n",
    "                                                   learning_decay, \n",
    "                                                   name='learning_rate')\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate, \n",
    "                                                      name='optimizer').minimize(loss, \n",
    "                                                                                 global_step=global_step)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = [tf.nn.softmax(model(tf_train_dataset)[i]\n",
    "                                         ,name=\"train_prediction_%d\" % i) for i in range(num_digits)]\n",
    "        \n",
    "        tf_valid_dataset = tf.constant(valid_dataset, name=\"tf_valid_dataset\")\n",
    "        valid_prediction = [tf.nn.softmax(model(tf_valid_dataset)[i]\n",
    "                                         ,name=\"valid_prediction_%d\" % i) for i in range(num_digits)]\n",
    "\n",
    "        tf_test_dataset = tf.constant(test_dataset, name=\"tf_test_dataset\")\n",
    "        test_prediction = [tf.nn.softmax(model(tf_test_dataset)[i]\n",
    "                                         ,name=\"test_prediction_%d\" % i) for i in range(num_digits)]\n",
    "        \n",
    "\n",
    "        # Create a single input tensor for use in the phone\n",
    "        tf_predict_single_dataset = tf.placeholder(tf.float32, shape=(1,\n",
    "                                                           img_height,\n",
    "                                                           img_width,\n",
    "                                                           3), name=\"tf_predict_single_dataset\")\n",
    "        # Average the color\n",
    "        conv = tf.reduce_mean(tf_predict_single_dataset, 3)\n",
    "        # Shape it correctly\n",
    "        conv = tf.reshape(conv, conv.get_shape().as_list() + [1])\n",
    "        # Move the color range\n",
    "        conv = tf.add(tf.div(conv, 255), -0.5)\n",
    "  \n",
    "        predict_single_output = tf.squeeze(tf.concat(1,\n",
    "                                          [tf.nn.softmax(model(conv)[i],\n",
    "                                                         name=\"test_prediction_%d\" % i) \n",
    "                                           for i in range(num_digits)]), name='predict_single_output')\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_graph(graph, train_dataset):\n",
    "    \n",
    "    tf_train_dataset = graph.get_tensor_by_name('tf_train_dataset:0')\n",
    "    tf_train_labels = [graph.get_tensor_by_name('tf_train_labels_%d:0' % i) for i in range(num_digits)]\n",
    "    train_prediction = [graph.get_tensor_by_name('train_prediction_%d:0' % i) for i in range(num_digits)]\n",
    "    valid_prediction = [graph.get_tensor_by_name('valid_prediction_%d:0' % i) for i in range(num_digits)]\n",
    "    test_prediction = [graph.get_tensor_by_name('test_prediction_%d:0' % i) for i in range(num_digits)]\n",
    "    optimizer = graph.get_tensor_by_name('optimizer:0')\n",
    "    loss = graph.get_tensor_by_name('loss:0')\n",
    "    learning_rate = graph.get_tensor_by_name('learning_rate:0')\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "\n",
    "            feed_dict = {tf_train_labels[i] : batch_labels[:, i, :] for i in range(num_digits)}\n",
    "            feed_dict[tf_train_dataset] = batch_data        \n",
    "\n",
    "\n",
    "            fetches = [optimizer, loss]\n",
    "            results = session.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            timeup = elapsed_time >= timeout\n",
    "            \n",
    "            if step > 0 or timeup:\n",
    "                if (step % eval_steps == 0 or timeup):\n",
    "                    print('Elapsed time(s):%d/%d (%.2f%%)' % (elapsed_time, timeout, 1.0 * elapsed_time / timeout))\n",
    "                    if timeup:\n",
    "                        print(\"\\nTIMEUP!\")\n",
    "                    print('Learning rate:', learning_rate.eval())\n",
    "                    print('Minibatch loss at step %d: %f' % (step, results[1]))\n",
    "\n",
    "                    fetches = train_prediction\n",
    "                    results = session.run(fetches, feed_dict={tf_train_dataset : batch_data})\n",
    "                    train_accuracy = accuracy_list(\n",
    "                        results,\n",
    "                        batch_labels)\n",
    "\n",
    "                    print('Minibatch accuracy: %.1f%%' % train_accuracy)\n",
    "            \n",
    "                if (step % valid_steps == 0 or timeup):\n",
    "                    valid_accuracy = accuracy_list(\n",
    "                        [valid_prediction[i].eval() for i in range(num_digits)],\n",
    "                        valid_labels)\n",
    "                    print('Validation accuracy: %.1f%%' % valid_accuracy)\n",
    "\n",
    "                if step % save_steps == 0 or timeup:\n",
    "                    test_accuracy = accuracy_list(\n",
    "                        [test_prediction[i].eval() for i in range(num_digits)],\n",
    "                        test_labels)\n",
    "                    print('Test accuracy: %.1f%%' % test_accuracy)\n",
    "                    \n",
    "            if timeup:\n",
    "                break\n",
    "\n",
    "        print(\"Saving graph\")\n",
    "        checkpoint_path = os.path.join('save', 'model.ckpt')\n",
    "        saver.save(session, checkpoint_path, global_step=0)\n",
    "        tf.train.write_graph(session.graph.as_graph_def(), 'save', 'model.pb')\n",
    "\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Elapsed time(s):17/60 (0.29%)\n",
      "Learning rate: 0.0498705\n",
      "Minibatch loss at step 500: 1.142597\n",
      "Minibatch accuracy: 63.8%\n",
      "Validation accuracy: 62.3%\n",
      "Elapsed time(s):42/60 (0.71%)\n",
      "Learning rate: 0.0497417\n",
      "Minibatch loss at step 1000: 1.325666\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 63.5%\n",
      "Elapsed time(s):60/60 (1.00%)\n",
      "\n",
      "TIMEUP!\n",
      "Learning rate: 0.0496601\n",
      "Minibatch loss at step 1317: 0.932562\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 64.3%\n",
      "Test accuracy: 67.8%\n",
      "Saving graph\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Basic\n",
    "test_graphs = False\n",
    "train_dataset, valid_dataset, test_dataset = dataset_28\n",
    "graph = create_graph(valid_dataset, test_dataset) \n",
    "run_graph(graph, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
