{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a basic driving agent\n",
    "**In your report, mention what you see in the agent’s behaviour. Does it eventually make it to the target location?**\n",
    "\n",
    "\n",
    "When I set the agent to choose a random action, it travels around randomly, disobeying the rules of the road and receiving many penalties. It eventually does make it to the destination. I would call this behaviour highly explorative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and update state\n",
    "**Justify why you picked these set of states, and how they model the agent and its environment.**\n",
    "\n",
    "\n",
    "I initially selected a large state space that used all available information. This included\n",
    "\n",
    "- Light colour\n",
    "\n",
    "- Car direction at each intersection\n",
    "\n",
    "- Next waypoint\n",
    "\n",
    "- Deadline\n",
    "\n",
    "I quickly realised that this state space has more information than is required for the agent to act in an acceptable fashion, so I refined it. Firstly I dropped the deadline as it has the largest impact on the state space size, and then I refined the information that required from the light and the intersection traffic . The state space became\n",
    "\n",
    "- W, the direction of the next waypoint. Values (wL|wR|wF|wX)\n",
    "\n",
    "- F, whether it's legal for the car to go forward. Values (F|xF)\n",
    "\n",
    "- L, whether it's legal for the car to turn left. Values (L|xL)\n",
    "\n",
    "- R, whether it's legal for the car to turn right. Values (R|xR)\n",
    "\n",
    "The above state space is small enough to allow for quick learning, and although I have implemented logic for legal turns, the agent will still need to learn how to use this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Q-Learning\n",
    "**What changes do you notice in the agent’s behaviour?**\n",
    "\n",
    "\n",
    "I implemented q-learning and made the agent chose the action that correlated with the highest q-value in the agents current state (s).\n",
    "\n",
    "\n",
    "In doing this the agent would often choose the same action, which generally was to turn right or stay still. This is because the agent would collect a positive reward from these actions, which would reinforce this same choice by increasing the q-values for them. When this happens we can say that the agent has found a local maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is the output after running the agent for 100 trials with the following variables\n",
    "\n",
    "\n",
    "*Gamma = 0.5; Learning Rate = 0.5; Chance of random action = 0.0; Q-value initialisation value = 0.0*\n",
    "\n",
    "    Complete:g:+0.500, lr:+0.500, rv:+0.000, qi:+0.000, dest:0, p:4, t:47.116, #finished:1\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    #Trials:100, Trial no:100, Step:42, Completions:0\n",
    "    Trial rewards:[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "    0.5, 0.5, 0.5, 0.5]\n",
    "                      State|,                                       Q-values |, Policy\n",
    "       ('wF', 'L', 'R', 'F'), ['N:+0.000', 'f:+0.000', 'l:+0.000', 'r:+1.120'], ['right']\n",
    "     ('wF', 'xL', 'R', 'xF'), ['N:+0.000', 'f:+0.000', 'l:+0.000', 'r:+1.126'], ['right']\n",
    "    ('wF', 'xL', 'xR', 'xF'), ['N:+0.000', 'f:+0.000', 'l:-0.220', 'r:+0.000'], [None, 'forward', 'right']\n",
    "       ('wL', 'L', 'R', 'F'), ['N:+0.000', 'f:+1.119', 'l:+0.000', 'r:+0.000'], ['forward']\n",
    "     ('wL', 'xL', 'R', 'xF'), ['N:+0.000', 'f:-0.191', 'l:+0.000', 'r:+1.069'], ['right']\n",
    "    ('wL', 'xL', 'xR', 'xF'), ['N:+1.238', 'f:+0.000', 'l:+0.000', 'r:+0.000'], [None]\n",
    "       ('wR', 'L', 'R', 'F'), ['N:+2.000', 'f:+0.000', 'l:+0.000', 'r:+0.000'], [None]\n",
    "      ('wR', 'xL', 'R', 'F'), ['N:+0.000', 'f:+0.000', 'l:+1.325', 'r:+0.000'], ['left']\n",
    "     ('wR', 'xL', 'R', 'xF'), ['N:+2.000', 'f:-0.158', 'l:+0.000', 'r:+0.000'], [None]\n",
    "    ('wR', 'xL', 'xR', 'xF'), ['N:+2.000', 'f:-0.002', 'l:+0.000', 'r:+0.000'], [None]\n",
    "\n",
    "We can see from this output that the trial rewards are all 0.5, and that the agent did not get to it's destination because there would be a higher reward at the end of this list.\n",
    "\n",
    "\n",
    "We can see that the agent did search the state space a little, this is indicated by the number of states in the q-matrix list. Here we can see that 10 states out of a possible 24 have been visited.\n",
    "\n",
    "\n",
    "On the far right we can see the policy of the agent, which describes the action the agent will chose if it is in that state. From the output it shows that the agent would\n",
    "\n",
    "- Turn right in 3 out of 10 states\n",
    "\n",
    "- Do nothing in 4 out of 10 states\n",
    "\n",
    "- Incur a penalty in state ('wR', 'xL', 'R', 'F') as it should not turn left\n",
    "\n",
    "- Probably incur a penalty in the state ('wF', 'xL', 'xR', 'xF')\n",
    "\n",
    "- Choose the wrong direction (i.e. not towards the next waypoint) in 5 out of 10 states.\n",
    "\n",
    "\n",
    "To summarise, the agent did not learn to reach the destination, but it did learn to avoid getting penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance the driving agent\n",
    "**Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?**\n",
    "\n",
    "\n",
    "I think the optimum policy is for an agent is to reach the destination with a minimal number of penalties.\n",
    "\n",
    "\n",
    "To have a better chance of achieving this, I used a number of independent variables to alter the way the agent behaved.\n",
    "\n",
    "\n",
    "**Gamma variable (g)**: Used in the Bellman equation to control the amount of information that is used from finding the max s_prime q-value. Range $0.0 <= g < 1.0$\n",
    "\n",
    "\n",
    "**Learning rate variable (lr)**: Used in the Bellman equation. This affects how much new information is stored over old information in the q-matrix. Range $0.0 <= lr < 1.0$\n",
    "\n",
    "\n",
    "**Q-initialisation variable (qi)**: The value is used to initialise new state space in the q-matrix. This affects how likely an agent is to explore the state space, as higher initial q-values would make the agent choose the action to try these new states. The range for this could be anything, but with our problem a range of $-5.0<=qi<5$ will suffice.\n",
    "\n",
    "\n",
    "**Random action variable (rv)**: A value that allows for random actions to happen. E.g. a random action value of 0.2 means that 20% of the time the agent will chose an action randomly. This affects how explorative the agent is and helps an agent to step out of local maximums. Range $0.0 <= rv < 1$\n",
    "\n",
    "### Choosing variable values\n",
    "There are many ways to discover the best values for these variables, and I have opted to run the agent for a number of samples with random values for the variables. The values chosen will be randomly distributed within the ranges discussed above.\n",
    "\n",
    "\n",
    "After taking the samples I will select the top 5% based on a score function, and use the means of this dataset for the variables in my optimal agent.\n",
    "\n",
    "\n",
    "For the scoring function, I have chosen this formula,\n",
    "$$s = \\sum(c) - \\left(\\frac{\\sum(p)}{t}\\right)^2$$\n",
    "where the score *s* is the sum of the number of times the agent reached the destination *c*, minus the average\n",
    "number of penalties squared. Where *p* indicates the number of penalties and *t* is the number of trials.\n",
    "\n",
    "\n",
    "Each sample and score will be calculated over 100 trials and I collected 6000 samples, here are the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               g         lr         rv         qi\n",
      "count  300.00000  300.00000  300.00000  300.00000\n",
      "mean     0.31003    0.52519    0.10123    2.47571\n",
      "std      0.20931    0.28875    0.06415    1.98333\n",
      "min      0.00042    0.00190    0.00177   -4.81933\n",
      "25%      0.14180    0.29725    0.05104    1.72514\n",
      "50%      0.28801    0.52547    0.09418    2.93091\n",
      "75%      0.45664    0.77819    0.14063    3.87140\n",
      "max      0.88973    0.99812    0.32781    4.99096\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "pd.set_option('precision', 5)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import os\n",
    "\n",
    "directory = \".\"\n",
    "filenames = [os.path.join(directory, f) for f in os.listdir(directory) if re.match(\".+t100_s1000.+\\.csv\", f)]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for filename in filenames:\n",
    "    # print \"Loading file:%s\" % filename\n",
    "    df_csv = pd.read_csv(filename)\n",
    "    df = pd.concat([df, df_csv], ignore_index=True)\n",
    "\n",
    "# Calculate the score as described above\n",
    "df[\"score\"] = df[\"r\"] - (df[\"p\"] / 100) ** 2\n",
    "\n",
    "# Take the top 5% of scores\n",
    "df_best = df[df.score > df.score.quantile(.95)]\n",
    "\n",
    "# Descrive the data\n",
    "print df_best[['g', 'lr', 'rv', 'qi']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimal agent\n",
    "I ran the agent with the variables set to the means taken from the above statistics.\n",
    "\n",
    "- Gamma = .31003\n",
    "\n",
    "- Learning rate = 0.52519\n",
    "\n",
    "- Random chance = 0.10123\n",
    "\n",
    "- Q-init value = 2.47571\n",
    "\n",
    "The output of this agent is shown here.\n",
    "\n",
    "    Complete:g:+0.310, lr:+0.525, rv:+0.101, qi:+2.476, dest:97, p:40, t:23.573, #finished:1\n",
    "    ------------------------------------------------------------------------------------------------------------\n",
    "    #Trials:100, Trial no:100, Step:14, Completions:97\n",
    "    Trial rewards:[1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 12]\n",
    "                      State|,                                       Q-values |, Policy\n",
    "       ('wF', 'L', 'R', 'F'), ['N:+2.753', 'f:+8.463', 'l:+2.713', 'r:+2.145'], ['forward']\n",
    "     ('wF', 'xL', 'R', 'xF'), ['N:+2.137', 'f:+0.478', 'l:+0.845', 'r:+1.854'], [None]\n",
    "    ('wF', 'xL', 'xR', 'xF'), ['N:+2.246', 'f:+2.184', 'l:+0.692', 'r:+2.310'], ['right']\n",
    "       ('wL', 'L', 'R', 'F'), ['N:+2.161', 'f:+1.739', 'l:+4.429', 'r:+1.578'], ['left']\n",
    "     ('wL', 'xL', 'R', 'xF'), ['N:+3.720', 'f:+0.394', 'l:+0.473', 'r:+1.916'], [None]\n",
    "       ('wR', 'L', 'R', 'F'), ['N:+2.515', 'f:+1.780', 'l:+1.841', 'r:+6.701'], ['right']\n",
    "     ('wR', 'xL', 'R', 'xF'), ['N:+2.476', 'f:+0.636', 'l:+2.048', 'r:+9.456'], ['right']\n",
    "    ('wR', 'xL', 'xR', 'xF'), ['N:+2.476', 'f:+2.476', 'l:+1.163', 'r:+2.476'], [None, 'forward', 'right']\n",
    "\n",
    "From these results we can see\n",
    "\n",
    "- From the top row, it shows the agent reached the destination in 97 out of 100 trials, (dest:97).\n",
    "\n",
    "- The agent was penalised 40 times, (p:40) in 100 trials.\n",
    "\n",
    "- In the last trial, the agent did not receive any penalties as the Trial_rewards are all positive.\n",
    "\n",
    "- In the last trial, the agent did reach the destination as we can see a reward of 12 as the last item.\n",
    "\n",
    "- The agent made it to 8 different states out of the possible 24 as there are 8 rows in the q-matrix.\n",
    "\n",
    "From these statistics we can say the agent has performed extremely well and has learnt very quickly, but has it learnt the optimum policy?\n",
    "\n",
    "    State          Policy                   Notes\n",
    "    wF  L  R  F => forward                  OK\n",
    "    wF xL  R xF => None                     OK\n",
    "    wF xL xR xF => right                    Penalties!\n",
    "    wL  L  R  F => left                     OK\n",
    "    wL xL  R xF => None                     OK\n",
    "    wR  L  R  F => right                    OK\n",
    "    wR xL  R xF => right                    OK\n",
    "    wR xL xR xF => None, forward, right     Possible penalties!\n",
    "\n",
    "We can see that the agent does not conform to the optimum policy as it is able to collect penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
